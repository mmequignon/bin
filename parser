#!/usr/bin/python3

import httplib2
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
import sys
import urllib.request


class Site:
    
    def __init__(self):
        self.url = self.getUrl()
        self.domain = self.getDomain()
        self.links = self.getLinks()
        self.pages = self.getPages()
        self.fileLinks = self.getFileLinks()

    def getUrl(self):
        url = None
        while not url:
            url=input("URL : ")
            if url[:-1]=="/":
                return url[:-1]
            else:
                return url

    def getDomain( self ):
        parsed_uri = urlparse(self.url)
        return parsed_uri.netloc

    def getLinks(self):
        return Page(self.url, self.url).links

    def getPages(self):
        array=[]
        self.links.append(self.url)
        for i in self.links:
            array.append(Page(i , self.url))
        list(set(array))
        return array

    def getFileLinks(self):

        array=[]
        for i in self.pages:
            array.extend(i.fileLinks)
        list(set(array))
        return array

    def download(self):
        for i in self.fileLinks:
            i.download()


class Page:
   
    def __init__(self, url, site):
        self.url = url
        self.site = site
        self.domain = self.getDomain()
        self.domainRegex = self.getDomainRegex()
        self.content = self.getContent()
        self.links = self.getLinks()
        self.fileLinks = self.getfileLinks()

    def getDomain(self):
        parsed_uri = urlparse(self.url)
        return parsed_uri.netloc

    def getDomainRegex(self):
        return r"http(s)?\:\/\/"+self.domain+".*"

    def getContent(self):
        try :
            http = httplib2.Http()
            status, response = http.request(self.url)
            return response
        except :
            print("403 : Parsing disallowed here !")
            raise

    def getLinks(self):
        array=[]
        soup = BeautifulSoup( self.content , "html.parser")
        for link in soup.find_all("a" , href=re.compile(self.domainRegex)):
            link = link.get("href")
            if File.extension(link) or link==self.url:
                continue
            array.append(link)
        list(set(array))
        return array

    def getfileLinks(self):
        array = []
        soup = BeautifulSoup( self.content , "html.parser")
        for link in soup.find_all("a", href=re.compile(".*\."+extension)):
            link = File.absolute(link.get("href"), self.site)
            array.append(File(link))
        list(set(array))
        return array


class File:

    def __init__(self, url):
        self.url = url
        self.name = self.getFileName()

    def getFileName(self):
        return self.url.split("/")[-1]

    def download(self):
        global fileName
        fileName = self.name       
        urllib.request.urlretrieve(self.url , self.name , reporthook=File.dlProgress)

    @staticmethod
    def dlProgress(count, blockSize, totalSize):
        percent = int(count*blockSize*100/totalSize)
        sys.stdout.write("\r" + "Downloading : " +  fileName + "...%d%%" % percent)
        sys.stdout.flush()

    @staticmethod
    def absolute(url , site):
        if not re.match( r"http(s)?\:\/\/.*" , url):
            if url[0]=="/" and site[-1]=="/":
                return site+url[1:]
            elif url[0]!="/" and site[-1]!="/":
                return site+"/"+url
            else :
                return site+url
        else:
            return url
    
    @staticmethod
    def extension(url):
        return re.match(r".*\."+extension , url )


global extension
extension = None
while not extension:
    extension = input( "Extension : ")

monSite = Site()
monSite.download()
